<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Neural Architecture Search | Amit Pant</title>
    <link>http://localhost:1313/tags/neural-architecture-search/</link>
      <atom:link href="http://localhost:1313/tags/neural-architecture-search/index.xml" rel="self" type="application/rss+xml" />
    <description>Neural Architecture Search</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Thu, 04 Apr 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu7729264130191091259.png</url>
      <title>Neural Architecture Search</title>
      <link>http://localhost:1313/tags/neural-architecture-search/</link>
    </image>
    
    <item>
      <title>FPGA-Optimized Neural Architecture Search for Enhanced Hardware Efficiency (FONAS)</title>
      <link>http://localhost:1313/project/fonas/</link>
      <pubDate>Thu, 04 Apr 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/project/fonas/</guid>
      <description>&lt;p&gt;Searched for the set of efficient deep neural architectures (FPGANets) for image classification with two constraints: arithmetic intensity and latency for FPGA. This approach outperformed many existing networks in terms of both latency and accuracy on ImageNet-1k.&lt;/p&gt;
&lt;!-- # FPGA-Optimized Neural Architecture Search for Enhanced Hardware Efficiency (FONAS)

**Summary:**  
Searched for the set of efficient deep neural architectures (FPGANets) for image classification with two constraints: arithmetic intensity and latency for FPGA. This approach outperformed many existing networks in terms of both latency and accuracy on ImageNet-1k.

[Github: FONAS](https://github.com/FPGA-Vision/FONAS)

## Project Work and Methodologies
- Compressing EfficientNet-V2 to implement it on FPGA.
- Leverage NAS techniques to automate the creation of task-specific neural networks targeting latency for FPGAs.
- **Hardware Optimization:** Focusing on co-designing models and hardware to enhance efficiency and performance.

## Hardware NAS Focus
- **Optimization Goals:** Minimizing latency, maximizing accuracy, and efficient resource utilization on FPGA platforms.
- **Architecture Sampling:** Generating diverse architectures meeting hardware constraints like latency and resource usage.
- **Evaluation Metrics:** Assessing performance based on accuracy, inference speed, and resource utilization for optimal architecture selection.

## Key Results and Findings
- **Compressed EfficientNet-V2:** Implemented channel pruning to reduce EfficientNetV2â€™s channel count by 88%, resulting in a model that was 14 times smaller, 2.5 times faster in inference speed, had 14 times fewer parameters, and 2.5 times fewer MAC operations.
- A **Latency dataset** for the Ultra96v2 FPGA board was constructed.
- **Latency-aware networks** and networks with varying arithmetic intensity were discovered through an evolutionary search process.
- The searched architectures (FPGANets) have better performance than most of the existing architectures in terms of the trade-off between latency and accuracy.

![Results](Assets/res.png)

## Future Directions
- **Integration Challenges:** Implementing optimized architectures on FPGA platforms seamlessly.
- **Validation Process:** Verifying the effectiveness of optimized architectures on Ultra96-v2 FPGA boards.
- **Framework Refinement:** Further enhancing the HW-NAS pipeline for improved efficiency and real-time performance.

This project aims to contribute significantly to the field of image classification by showcasing the benefits of HW-NAS and FPGA-based acceleration in achieving superior efficiency and real-time performance metrics. --&gt;</description>
    </item>
    
  </channel>
</rss>
